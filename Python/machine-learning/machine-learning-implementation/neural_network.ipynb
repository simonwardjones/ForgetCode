{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import graphviz\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.offline as py\n",
    "from graphviz import Digraph\n",
    "from IPython.display import display\n",
    "from plotly import graph_objects as go\n",
    "from scipy.special import logsumexp, expit\n",
    "from sklearn.datasets import load_boston, load_iris\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import export_graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### The maths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A neural network mimics the brain as a series of nodes structured in layers. The network passes information between the layers depending on the layer's weights and the outputs from the previous layer.\n",
    "\n",
    "Let's assume we have data $x_1, x_2, \\dots x_m \\in \\mathbb{R}^{(n^0,1)}$ where m is the number of training examples and $n^0$ is the number of features in $x_i$\n",
    "Let's assume a classification problem with $y_1, y_2, \\dots y_m \\in{0,1}$\n",
    "\n",
    "Define $X$ with shape $(m, n^0)$ by stacking $x_1, x_2, \\dots x_m$ as\n",
    "$$\n",
    "X = \\begin{pmatrix}\n",
    "- & x_1^T & -\\\\\n",
    "- & x_2^T & -\\\\\n",
    "&\\vdots& \\\\\n",
    "- & x_m^T & -\n",
    "\\end{pmatrix}\t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We define the network with $L$ layers where the $L$'th layer is the output layer and layer $0$ is the input layer. The layers $1, 2, \\dots, L-1$ are known as the hidden layers. We define $n^l$ to be the number of nodes in layer $l$ for $l \\in 0, 1, \\dots, L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Feed forwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For a fully connected network the input to a node is equal to the sum of the output from each of the nodes in the previous layer multiplied by the weights for that layer plus a bias.  \n",
    "The output of the node is calculated by transforming the input using the activation function $g$. Various activation functions can be used but often it is chosen to be the sigmoid function, the relu function or the tanh function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For node $i$ in layer $l$ where $i\\in 1,\\dots n^l$ we define the net input $z^l_i$ and output $a^l_i$ as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "z^l_i&=\\sum^{n^{l-1}}_{j=0}W^l_{ij}a^{l-1}_j + \\beta^l_i \\quad i\\in 1,\\dots, n^l\\quad(1)\\\\\n",
    "a^l_i&=g(z^l_i) \\quad i\\in 1,\\dots, n^l\\quad(2)\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note weight $W_{ij}^l$ goes from $a_j^{l-1} \\to z_i^{l}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's assume that the activation function $g$ is the sigmoid function then "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "g(x)&=\\frac{1}{1+e^{-x}}\\quad(3)\\\\\n",
    "g'(x)&=g(x)(1-g(x))\\quad(4)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Feed forward as vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "z^l &= W^l*a^{l-1} + \\beta^l\\quad(5)\\\\\n",
    "a^l &= g(z^l)\\quad(6)\\\\\n",
    "\\end{align}\\\\\n",
    "\\quad z^l,\\beta^l\\in \\mathbb{R}^{(n^l, 1)}\n",
    "\\quad a^{l-1}\\in \\mathbb{R}^{(n^{l-1}, 1)}\n",
    "\\quad W^{l-1} \\in \\mathbb{R}^{(n^{l}, n^{l-1})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Feed forward as matrices! Performing on multiple samples $x_i$ at the same time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "Z^l &= W^l*A^{l-1} + B^l\\quad(7)\\\\\n",
    "A^l &= g(Z^l)\\quad(8)\\\\\n",
    "Z^1 &= W^1*X^{T} + B^l\\quad(9)\\\\\n",
    "\\end{align}\\\\\n",
    "\\quad Z^l, B^l\\in \\mathbb{R}^{(n^l, m)}\n",
    "\\quad a^{l-1}\\in \\mathbb{R}^{(n^{l-1}, m)}\n",
    "\\quad W^{l-1} \\in \\mathbb{R}^{(n^{l}, n^{l-1})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note the columns of $Z^l$ and $A^l$ relate to $z^l$ and $a^l$  for the individual samples $x_k \\quad k\\in 1\\dots m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "Z^l = \\begin{pmatrix}\n",
    "| & | & \\dots & |\\\\\n",
    "z^l_{x_1} & z^l_{x_2} & \\dots & z^l_{x_m}\\\\\n",
    "| & | & \\dots & |\n",
    "\\end{pmatrix}\n",
    "\\quad\n",
    "A^l = \\begin{pmatrix}\n",
    "| & | & \\dots & |\\\\\n",
    "a^l_{x_1} & a^l_{x_2} & \\dots & a^l_{x_m}\\\\\n",
    "| & | & \\dots & |\n",
    "\\end{pmatrix}\t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The m columns of $B^l$ are all copies of $\\beta^l$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "B^l = \\begin{pmatrix}\n",
    "| & | & \\dots & |\\\\\n",
    "\\beta^l & \\beta^l & \\dots & \\beta^l\\\\\n",
    "| & | & \\dots & |\n",
    "\\end{pmatrix}\t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Back propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the binary classification context we can define the loss a.k.a cost function as the cross entropy (for one input data item $x_i$ and true output $y_i$ where $a^L_1$ is the model prediction of $y_i$ being in class 1 or $P(y\\in C_1)$ the loss is defined as follows:  \n",
    "\n",
    "$$\n",
    "\\mathcal{L} = y_i\\log(a^L_1)+(1-y_i)\\log(1-a^L_{1})\\quad(10)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note this is just the loss for one given sample $x_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As in all the other algorithms we seek to improve the model by changing the weights to reduce the loss. We do this by stepping the weights in the direction of the negative gradient of the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "W^l_{ij} &\\to W^l_{ij} + \\eta \\frac{\\partial \\mathcal{L}}{\\partial W^l_{ij}} \n",
    "\\quad i\\in 1,\\dots, n^l\n",
    "\\quad j\\in 1,\\dots, n^{l-1}\n",
    "\\quad l\\in 1,\\dots, L \\quad(11)\\\\\n",
    "\\beta^l_{i} &\\to \\beta^l_{i} + \\eta \\frac{\\partial \\mathcal{L}}{\\partial \\beta^l_{i}} \n",
    "\\quad i\\in 1,\\dots, n^l\n",
    "\\quad l\\in 1,\\dots, L\\quad(12)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Back propagation is involved but ultimately comes down to calculating $\\frac{\\partial \\mathcal{L}}{\\partial W^l_{ij}} $ and $\\frac{\\partial \\mathcal{L}}{\\partial \\beta^l_{i}}$.  \n",
    "As the name back propagation implies we calculate the derivatives starting at the end or the \"right\" of the network and propagate the error backwards (using the chain rule).\n",
    "\n",
    "Above we have defined the loss for one training example. In this way we can updated the weights in a stochastic manner - or we can average the error across multiple samples and used batch gradient descent or just regular gradient descent if we average across all samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Calculating $\\frac{\\partial \\mathcal{L}}{\\partial W^l_{ij}} $ and $\\frac{\\partial \\mathcal{L}}{\\partial \\beta^l_{i}}$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note for each layer $l$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W^l_{ij}} &= \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial z^l_{i}}\n",
    "\\frac{\\partial z^l_{i}}{\\partial W^l_{ij}}\n",
    ", \\quad\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\beta^l_{i}} = \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial z^l_{i}}\n",
    "\\frac{\\partial z^l_{i}}{\\partial \\beta^l_{i}}\\quad(13)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "or defining $\\delta^l_i=\\frac{\\partial \\mathcal{L}}{\\partial z^l_{i}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W^l_{ij}} &= \n",
    "\\delta^l_i\n",
    "\\frac{\\partial z^l_{i}}{\\partial W^l_{ij}}\n",
    ", \\quad\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\beta^l_{i}} = \n",
    "\\delta^l_i\n",
    "\\frac{\\partial z^l_{i}}{\\partial \\beta^l_{i}}\\quad(14)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We know ( by differentiating equation (1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial z^l_{i}}{\\partial W^l_{ij}} = a^{l-1}_j\n",
    ",\\quad\n",
    "\\frac{\\partial z^l_{i}}{\\partial \\beta^l_{i}} = 1\n",
    "\\quad(15)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So in order to calculate the derivatives we must calculate $\\delta^l_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For the very last layer (when there is only one node in the output layer) we can use the chain rule to write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W^L_{1j}} &= \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial z^L_{1}}\n",
    "\\frac{\\partial z^L_{1}}{\\partial W^L_{1j}} = \n",
    "\\delta^L_1\n",
    "\\frac{\\partial z^L_{1}}{\\partial W^L_{1j}}\\quad(16)\\\\\n",
    "\\delta^L_1 &= \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial a^L_{1}}\n",
    "\\frac{\\partial a^L_{1}}{\\partial z^L_{1}}\n",
    "\\quad(17)\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial a^L_{1}} &= \n",
    "\\frac{\\partial}{\\partial a^L_{1}}\\left(\n",
    "y_i\\log(a^L_1)+(1-y_i)\\log(1-a^L_{1})\n",
    "\\right)\\quad(18)\\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial a^L_{1}} &= \n",
    "\\frac{y_i}{a^L_1} + \\frac{1-y_i}{1-a^L_1}\\quad(19)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Assuming sigmoid activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial a^L_{i}}{\\partial z^L_{i}} &=\n",
    "\\frac{\\partial}{\\partial z^L_{i}}\\left(\n",
    "g(z^L_{i})\n",
    "\\right) = \n",
    "\\frac{\\partial}{\\partial z^L_{i}}\\left(\n",
    "\\sigma(z^L_{i})\n",
    "\\right)\\\\\n",
    "&= \\sigma'(z^L_{i})\\\\\n",
    "&= \\sigma(z^L_{i})(1-\\sigma(z^L_{i}))\\\\\n",
    "&= a^L_{i}*(1-a^L_{i})\\quad(20)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Based on equations (19) and (20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\delta^L_1 = a^L_1 - y\\quad(21)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Calculating $\\delta^l_i$ for $l<L$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\delta^l_i &= \\frac{\\partial \\mathcal{L}}{\\partial z^l_{i}} \\\\\n",
    "\\delta^l_i &= \\frac{\\partial \\mathcal{L}}{\\partial a^l_{i}}\n",
    "\\frac{\\partial a^l_{i}}{\\partial z^l_{i}}\\\\\n",
    "\\delta^l_i &= \\left(\n",
    "\\sum_{j=1}^{n^{l+1}}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial z^{l+1}_{j}}\n",
    "\\frac{\\partial z^{l+1}_{j}}{\\partial a^l_{i}}\n",
    "\\right)\n",
    "\\frac{\\partial a^l_{i}}{\\partial z^l_{i}}\\quad(22)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Differentiating (1) to get $\\frac{\\partial z^{l+1}_{j}}{\\partial a^l_{i}}$ and using equation (20) we can see"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\delta^l_i &= \\left(\n",
    "\\sum_{j=1}^{n^{l+1}}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial z^{l+1}_{j}}\n",
    "W_{ji}^{l+1}\n",
    "\\right)\n",
    "a^l_{i}*(1-a^l_{i})\\quad(23)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "And that's it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Or we can do it again using vectors/matrices instead of the scalar derivation above. Equations (11) and (12) become\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "W^l &\\to W^l - \\eta \\frac{\\partial \\mathcal{L}}{\\partial W^l} \n",
    "\\quad l\\in 1,\\dots, L \\quad(24)\\\\\n",
    "\\beta^l &\\to \\beta^l - \\eta \\frac{\\partial \\mathcal{L}}{\\partial \\beta^l} \n",
    "\\quad l\\in 1,\\dots, L\\quad(25)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Equation (23) becomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\delta^l &= (W^{l+1})^T\\delta^{l+1}\\odot a^l\\odot(1-a^l)\\quad(26)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Hence\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W^l}  = \\delta^l(a^{l-1})^T\\quad\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\beta^l}  = \\delta^l\\quad(27)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Back propagation as matrices! Performing on multiple samples $x_i$ at the same time using $X$. This is the batch approach to gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If we define $Y$ with shape $(n^L, m) = (1, m)$ as "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "Y = \\begin{pmatrix}\n",
    "| & | & \\dots & |\\\\\n",
    "y_1 & y_2 & \\dots & y_m\\\\\n",
    "| & | & \\dots & |\n",
    "\\end{pmatrix}\t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L} = \\frac{1}{m}\\sum_{k=1}^{m}Y_{1k}\\log(A^L_{1k})+(1-Y_{1k})\\log(1-A^L_{1k})\n",
    "\\quad(28)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We define \n",
    "$$D^L = A^L - Y = \n",
    "\\begin{pmatrix}\n",
    "| & | & \\dots & |\\\\\n",
    "\\delta^l_{x_1} & \\delta^l_{x_2} & \\dots & \\delta^l_{x_m}\\\\\n",
    "| & | & \\dots & |\n",
    "\\end{pmatrix} \\in \\mathbb{R}^{(n^{L}, m)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "D^l = (W^{l+1})^TD^{l+1}\\odot A^l\\odot(\\mathbf{1}^{(n^l,m)}-A^l)\n",
    "\\quad\\in \\mathbb{R}^{(n^{l}, m)} \\quad l\\in 1\\dots L-1\n",
    "\\quad(29)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Then the updates are \n",
    "$$\n",
    "\\begin{align}\n",
    "W^l &\\to W^l - \\frac{\\eta}{m}D^l(A^{l-1})^T\\quad(30)\\\\\n",
    "\\beta^l &\\to \\beta^l - \\frac{\\eta}{m}D^l\\mathbf{1}^{(m,1)}\\quad(31)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Extending to regression and mulitclass classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In both of these cases we need to make two changes \n",
    " 1. We change the activation function in the final layer\n",
    " 2. We change the cost function and so need to re-calculate $\\delta^L$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. We change the activation function in the final layer to simply be the identity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "a^L_{1} = g(z^L_{1}) = z^L_{1} \\implies \\frac{\\partial a^L_{1}}{\\partial z^L_{1}} = 1\n",
    "\\quad(32)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "2. We define the cost function for one sample $x$ and corresponding $y$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\mathcal{L} = \\frac{1}{2}(y - a^L_{1})^2\\quad(33)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We now need to update update equation (17) using (32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\delta^L_1 &= \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial a^L_{1}}\n",
    "\\frac{\\partial a^L_{1}}{\\partial z^L_{1}}\n",
    "\\quad(34)\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\delta^L_1 \n",
    "= \\frac{\\partial a^L_{1}}{\\partial z^L_{1}}\n",
    "=\\frac{\\partial \\mathcal{L}}{\\partial a^L_{1}}\n",
    "\\quad(35)\n",
    "$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\delta^L_1 = \\frac{\\partial \\mathcal{L}}{\\partial z^L_{1}} = a^L_{1} - y\\quad(36)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note this is the same as equation (21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Using the matrices above\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{2m}\\sum_{k=1}^{m}(Y_{1k} - A^L_{1k})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Multiclass classification (number classes > 2)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. We change the activation function in the final layer to be the softmax function. Note that each sample $x_i$ has corresponding $y_i$ with shape $(n^L,1)$. Also note the number of classes is $n^L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "a^L_i &= \\sigma_i(z^L_1, z^L_2, \\dots, z^L_{n^l})\\\\\n",
    "&= \\frac{e^{z^L_i}}{\\sum_{r=1}^{n^L}e^{z^L_r}}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "2. Defining the cost function (for sample $x$ and corresponding $y$), and again we update equation (17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\mathcal{L} = -\\sum_{r=1}^{n^L}y_r\\log(a^L_r)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "hence substituting for $a^L_r$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L} \n",
    "&= -\\sum_{r=1}^{n^L}y_r\\log\\left(\n",
    "\\frac{e^{z^L_r}}{\\sum_{s=1}^{n^L}e^{z^L_s}}\n",
    "\\right)\\\\\n",
    "&= -\\sum_{r=1}^{n^L}\n",
    "\\left(\n",
    "y_r\\log(e^{z^L_r}) - y_r\\log({\\sum_{s=1}^{n^L}e^{z^L_s}})\n",
    "\\right)\\\\\n",
    "&= -\\sum_{r=1}^{n^L}y_r z^L_r - \\log({\\sum_{s=1}^{n^L}e^{z^L_s}})\\\\\n",
    "&= \\log({\\sum_{s=1}^{n^L}e^{z^L_s}}) - \\sum_{r=1}^{n^L}y_r z^L_r\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "hence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\delta^L_i \n",
    "= \\frac{\\partial \\mathcal{L}}{\\partial z^L_{1}}\n",
    "= \\frac{e^{z^L_i}}{\\sum_{r=1}^{n^L}e^{z^L_r}} - y_i = a^L_i - y_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note this is the same as equation (21) again!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Using the matrices above\n",
    "$$\n",
    "\\mathcal{L} = -\\frac{1}{2m}\\sum_{k=1}^{m}\\sum_{r=1}^{n^L}\n",
    "Y_{rk}\\log(A^L_{rk})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Define the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# You can ignore the random logging metaclass\n",
    "\n",
    "class LoggedClassMeta(type):\n",
    "    def __init__(cls, name, bases, dct,**kwargs):\n",
    "        #  print(cls, name, bases, dct,kwargs)\n",
    "        if '_logging_level' in dct:\n",
    "            level = dct['_logging_level']\n",
    "        else:\n",
    "            level = logging.INFO\n",
    "        logger = logging.getLogger(name)\n",
    "        logger.setLevel(level)\n",
    "        cls.logger = logger\n",
    "\n",
    "        \n",
    "class LoggedClass(metaclass=LoggedClassMeta):\n",
    "    pass\n",
    "\n",
    "\n",
    "logging.basicConfig()\n",
    "\n",
    "\n",
    "class NeuralNetwork(LoggedClass):\n",
    "    _logging_level = logging.INFO\n",
    "    \n",
    "    def __init__(self,\n",
    "                 layer_sizes=[5,10,1],\n",
    "                 is_classifier=True,\n",
    "                 learning_rate=0.1):\n",
    "        self.layer_sizes = layer_sizes # n^0,...,n^L above\n",
    "        self.is_classifier = is_classifier\n",
    "        self.learning_rate = learning_rate  # eta above\n",
    "        self.n_L = layer_sizes[-1]  # n^L above\n",
    "        self.n_layers = len(layer_sizes) - 1  # L above\n",
    "        self.initialise_weights()\n",
    "        \n",
    "    def initialise_weights(self):\n",
    "        self.weight_matrices = [\n",
    "            np.random.normal(loc=0.0, scale=1.0, size=(n_l, n_l_minus_1))\n",
    "            for n_l, n_l_minus_1 in zip(self.layer_sizes[1:], self.layer_sizes)\n",
    "        ]\n",
    "        self.betas = [np.zeros(shape=(n_l, 1)) for n_l in self.layer_sizes[1:]]\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        m = X.shape[0]\n",
    "        layer_activations = [X.T]\n",
    "        for layer in range(self.n_layers):\n",
    "            A_layer_minus_1 = layer_activations[-1]\n",
    "            beta = self.betas[layer]\n",
    "            B = np.repeat(beta, m, axis=-1)\n",
    "            Z = self.weight_matrices[layer] @ A_layer_minus_1 + B\n",
    "            A = self.activation_function(Z, layer=layer)\n",
    "            layer_activations.append(A)\n",
    "            self.log_layer(layer, A_layer_minus_1, beta, B, Z, A)\n",
    "        self.layer_activations = layer_activations\n",
    "        return layer_activations[-1]\n",
    "\n",
    "    def back_propogation(self, X, Y):\n",
    "        assert X.shape[0] == Y.shape[1]\n",
    "        final_layer_error = self.layer_activations[-1] - Y\n",
    "        D_plus_1 = final_layer_error\n",
    "        # errors represent D matrices\n",
    "        errors = [D_plus_1]\n",
    "        for layer in range(self.n_layers - 2, -1, -1):\n",
    "            self.logger.debug(f'Calculating D_{layer + 1}')\n",
    "            A = self.layer_activations[layer + 1]\n",
    "            self.log_back_prop_layer(layer, A, D_plus_1)\n",
    "            D = (self.weight_matrices[layer + 1].T @ D_plus_1) * \\\n",
    "                A * (1 - A)\n",
    "            D_plus_1 = D\n",
    "            errors.insert(0, D)\n",
    "        self.errors= errors\n",
    "        self.update_weights()\n",
    "\n",
    "    def update_weights(self):\n",
    "        for layer in range(self.n_layers):\n",
    "            m = self.errors[0].shape[1]\n",
    "            d_L_d_W = (1 / m) * self.errors[layer] @ \\\n",
    "                self.layer_activations[layer].T\n",
    "            d_L_d_beta = (1 / m) * self.errors[layer].sum(axis=1)[:, None]\n",
    "            self.weight_matrices[layer] = self.weight_matrices[layer] - \\\n",
    "                self.learning_rate * d_L_d_W\n",
    "            if layer==0:\n",
    "                self.d_L_d_Ws.append(d_L_d_W.sum())\n",
    "            self.betas[layer] = self.betas[layer] - \\\n",
    "                self.learning_rate * d_L_d_beta\n",
    "            \n",
    "    def log_layer(self, layer, A_layer_minus_1, beta, B, Z, A):\n",
    "        self.logger.debug(\n",
    "            f'A_layer_minus_1 i.e. A_{layer} '\n",
    "            f'has shape {A_layer_minus_1.shape}')\n",
    "        self.logger.debug(f'beta_{layer + 1} has shape {beta.shape}')\n",
    "        self.logger.debug(f'B_{layer + 1} has shape {B.shape}')\n",
    "        self.logger.debug(f'Z_{layer + 1} has shape {Z.shape}')\n",
    "        self.logger.debug(f'A_{layer + 1} has shape {A.shape}')\n",
    "    \n",
    "    def log_back_prop_layer(self, layer,  A, D_plus_1):\n",
    "        self.logger.debug(\n",
    "            f'A_{layer + 1} has shape {A.shape}')\n",
    "        self.logger.debug(\n",
    "            f'W_{layer + 2} has shape '\n",
    "            f'{self.weight_matrices[layer + 1].shape}')\n",
    "        self.logger.debug(\n",
    "            f'D_{layer + 2} has shape {D_plus_1.shape}')\n",
    "\n",
    "    def activation_function(self, Z, layer):\n",
    "        if layer == (self.n_layers -1):\n",
    "            if not self.is_classifier:\n",
    "                return Z\n",
    "            if self.is_classifier and self.n_L >= 2:\n",
    "                return np.exp(Z - logsumexp(Z, axis=0)[None,:])\n",
    "        return expit(Z)\n",
    "    \n",
    "    def cost(self, Y):\n",
    "        if self.is_classifier and self.n_L == 1:\n",
    "            cost = (-1 / m) * (\n",
    "                Y * np.log(self.layer_activations[-1]) + \\\n",
    "                (1 - Y) * np.log(1 - self.layer_activations[-1])\n",
    "            ).sum()\n",
    "        if self.is_classifier and self.n_L > 1:\n",
    "            cost = (-1 / m) * \\\n",
    "                (Y * np.log(self.layer_activations[-1])).sum()\n",
    "        if not self.is_classifier:\n",
    "            cost = (1 / (2 * m)) * \\\n",
    "                ((Y - self.layer_activations[-1]) ** 2).sum()\n",
    "        self.logger.debug(f'cost = {cost}')\n",
    "        self.costs.append(cost)\n",
    "    \n",
    "    def fit(self, X, Y, epochs=100):\n",
    "        if self.n_L > 1:\n",
    "            if Y.shape[0] != self.n_L:\n",
    "                print('One hot encoding Y')\n",
    "                Y = np.eye(self.n_L)[:,Y.reshape(-1).astype(int)]\n",
    "        self.costs = []\n",
    "        self.d_L_d_Ws = []\n",
    "        for epoch in range(epochs):\n",
    "            self.feed_forward(X)\n",
    "            self.cost(Y)\n",
    "            self.back_propogation(X, Y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        A_L = self.feed_forward(X)\n",
    "        if not self.is_classifier:\n",
    "            return A_L\n",
    "        if self.is_classifier and self.n_L == 1:\n",
    "            return np.round(A_L).astype(int)\n",
    "        if self.is_classifier and self.n_L > 1:\n",
    "            return np.argmax(A_L, axis=0)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        A_L = self.feed_forward(X)\n",
    "        if not self.is_classifier:\n",
    "            raise Exception('Must be a classifier')\n",
    "        return A_L\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Neural network on toy example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Make fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "m = 20\n",
    "X = np.linspace(0, 10, m).reshape((m, 1))\n",
    "epsilon =  np.random.normal(scale=2, size=(m, 1))\n",
    "Y = X ** 2 + X + epsilon\n",
    "Y = (Y > 30).astype(int).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Fit network and visualise cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "neural_network = NeuralNetwork(\n",
    "    layer_sizes=[1,3,1],\n",
    "    learning_rate=0.3)\n",
    "neural_network.fit(X, Y, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bda851ce4254b56bfb42022a9e35beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'type': 'scatter',\n",
       "              'uid': '02077f81-e63e-4370-9ad3-9c28a97c3ef8',\n",
       " …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = go.FigureWidget()\n",
    "fig.add_scatter(\n",
    "    x=list(range(len(neural_network.costs))),\n",
    "    y=neural_network.costs)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 with val 0.247, cost: 0.730\n",
      "iteration 100 with val 0.029, cost: 0.372\n",
      "iteration 200 with val 0.001, cost: 0.206\n",
      "iteration 300 with val 0.001, cost: 0.142\n",
      "iteration 400 with val 0.001, cost: 0.110\n"
     ]
    }
   ],
   "source": [
    "for i, (val,cost) in enumerate(zip(neural_network.d_L_d_Ws,neural_network.costs)):\n",
    "    if i % 100 == 0:\n",
    "        print(f'iteration {i} with val {abs(val):.3f}, cost: {cost:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network.predict_proba(X)\n",
    "neural_network.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Neural network binary classifier - Titanic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Load titanic data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_feather('../titanic/processed/X_train.feather')\n",
    "y_train = pd.read_feather('../titanic/processed/y_train.feather')\n",
    "X_test = pd.read_feather('../titanic/processed/X_test.feather')\n",
    "y_test = pd.read_feather('../titanic/processed/y_test.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Neural network model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 78.77%\n"
     ]
    }
   ],
   "source": [
    "titanic_nn = NeuralNetwork(layer_sizes=[30,50,1], learning_rate=0.5)\n",
    "titanic_nn.fit(X_train.values, y_train.values.T, epochs=400)\n",
    "y_pred = titanic_nn.predict(X_test.values)\n",
    "test_acc = (y_pred == y_test.values.flatten()).sum() / len(y_test)\n",
    "print(f'Test accuracy = {test_acc:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faee6e205d364f1eb074545fb2ad58cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'type': 'scatter',\n",
       "              'uid': '09a1b12b-8192-4c46-90f1-7af1496cf419',\n",
       " …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = go.FigureWidget()\n",
    "fig.add_scatter(\n",
    "    x=list(range(len(titanic_nn.costs))),\n",
    "    y=titanic_nn.costs)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Neural network classifier - Iris data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Load the iris data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  y\n",
       "0                6.1               2.8                4.7               1.2  1\n",
       "1                5.7               3.8                1.7               0.3  0\n",
       "2                7.7               2.6                6.9               2.3  2\n",
       "3                6.0               2.9                4.5               1.5  1\n",
       "4                6.8               2.8                4.8               1.4  1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_data = load_iris()\n",
    "iris_df = pd.DataFrame(iris_data['data'],columns=iris_data['feature_names'])\n",
    "iris_df['y'] = iris_data['target']\n",
    "iris_df = iris_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "iris_sample = iris_df.head(5)\n",
    "iris_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Fit neural network classifier and visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One hot encoding Y\n"
     ]
    }
   ],
   "source": [
    "iris_sample_vals = iris_sample.values\n",
    "\n",
    "# # for small sample\n",
    "# X = iris_sample_vals[:,:-1]\n",
    "# y = iris_sample_vals[:,-1]\n",
    "\n",
    "X = iris_df.values[:,:-1]\n",
    "m = X.shape[0]\n",
    "y = iris_df.values[:,-1].reshape(1, m)\n",
    "\n",
    "iris_neural_network = NeuralNetwork(layer_sizes=[4,8,3])\n",
    "iris_neural_network.fit(X, y, epochs=800)\n",
    "\n",
    "feature_names = iris_data['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b893ab2a328440d3b7b508968b95ddef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'type': 'scatter',\n",
       "              'uid': 'c9bca653-9f6f-4b44-875c-1843768321cf',\n",
       " …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = go.FigureWidget()\n",
    "fig.add_scatter(\n",
    "    x=list(range(len(iris_neural_network.costs))),\n",
    "    y=iris_neural_network.costs)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Example prediction on Iris data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_neural_network.predict(X[:4,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 2., 1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0244518 , 0.96999122, 0.00134162, 0.03084734],\n",
       "       [0.80990323, 0.02856427, 0.04705977, 0.75291137],\n",
       "       [0.16564497, 0.00144451, 0.95159861, 0.2162413 ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_neural_network.predict_proba(X)[:,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n",
       "       0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0,\n",
       "       0, 1, 2, 2, 1, 2, 1, 2, 1, 0, 2, 1, 0, 0, 0, 1, 2, 0, 0, 0, 1, 0,\n",
       "       1, 2, 0, 1, 2, 0, 2, 2, 1, 2, 2, 1, 0, 1, 2, 0, 0, 1, 1, 0, 2, 0,\n",
       "       0, 2, 1, 2, 2, 2, 2, 1, 0, 0, 2, 2, 0, 0, 0, 1, 2, 0, 2, 2, 0, 1,\n",
       "       1, 2, 1, 2, 0, 2, 1, 2, 1, 1, 1, 0, 1, 1, 0, 1, 2, 2, 0, 1, 2, 2,\n",
       "       0, 2, 0, 1, 2, 2, 1, 2, 1, 1, 2, 2, 0, 1, 2, 0, 1, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_neural_network.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n",
       "        0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0,\n",
       "        0, 1, 2, 2, 1, 2, 1, 2, 1, 0, 2, 1, 0, 0, 0, 1, 2, 0, 0, 0, 1, 0,\n",
       "        1, 2, 0, 1, 2, 0, 2, 2, 1, 1, 2, 1, 0, 1, 2, 0, 0, 1, 1, 0, 2, 0,\n",
       "        0, 1, 1, 2, 1, 2, 2, 1, 0, 0, 2, 2, 0, 0, 0, 1, 2, 0, 2, 2, 0, 1,\n",
       "        1, 2, 1, 2, 0, 2, 1, 2, 1, 1, 1, 0, 1, 1, 0, 1, 2, 2, 0, 1, 2, 2,\n",
       "        0, 2, 0, 1, 2, 2, 1, 2, 1, 1, 2, 2, 0, 1, 2, 0, 1, 2]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(iris_neural_network.predict(X) == y.astype(int)).sum() / y.shape[1]\n",
    "# Likely overfitting as no regularization!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Neural network regressor - Boston housing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Load Boston housing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.09178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.510</td>\n",
       "      <td>6.416</td>\n",
       "      <td>84.1</td>\n",
       "      <td>2.6463</td>\n",
       "      <td>5.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>395.50</td>\n",
       "      <td>9.04</td>\n",
       "      <td>23.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.05644</td>\n",
       "      <td>40.0</td>\n",
       "      <td>6.41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.447</td>\n",
       "      <td>6.758</td>\n",
       "      <td>32.9</td>\n",
       "      <td>4.0776</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>17.6</td>\n",
       "      <td>396.90</td>\n",
       "      <td>3.53</td>\n",
       "      <td>32.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.10574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.983</td>\n",
       "      <td>98.8</td>\n",
       "      <td>1.8681</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>390.11</td>\n",
       "      <td>18.07</td>\n",
       "      <td>13.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.09164</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.413</td>\n",
       "      <td>6.065</td>\n",
       "      <td>7.8</td>\n",
       "      <td>5.2873</td>\n",
       "      <td>4.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>390.91</td>\n",
       "      <td>5.52</td>\n",
       "      <td>22.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.09017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.713</td>\n",
       "      <td>6.297</td>\n",
       "      <td>91.8</td>\n",
       "      <td>2.3682</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>385.09</td>\n",
       "      <td>17.27</td>\n",
       "      <td>16.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS   RAD    TAX  \\\n",
       "0  0.09178   0.0   4.05   0.0  0.510  6.416  84.1  2.6463   5.0  296.0   \n",
       "1  0.05644  40.0   6.41   1.0  0.447  6.758  32.9  4.0776   4.0  254.0   \n",
       "2  0.10574   0.0  27.74   0.0  0.609  5.983  98.8  1.8681   4.0  711.0   \n",
       "3  0.09164   0.0  10.81   0.0  0.413  6.065   7.8  5.2873   4.0  305.0   \n",
       "4  5.09017   0.0  18.10   0.0  0.713  6.297  91.8  2.3682  24.0  666.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT     y  \n",
       "0     16.6  395.50   9.04  23.6  \n",
       "1     17.6  396.90   3.53  32.4  \n",
       "2     20.1  390.11  18.07  13.6  \n",
       "3     19.2  390.91   5.52  22.8  \n",
       "4     20.2  385.09  17.27  16.1  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_data = load_boston()\n",
    "boston_df = pd.DataFrame(boston_data['data'], columns=boston_data['feature_names'])\n",
    "boston_df['y'] = boston_data['target']\n",
    "boston_df = boston_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "boston_sample = boston_df.head(5)\n",
    "boston_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Fit neural network on Boston data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = boston_df.values[:,:-1]\n",
    "m = X.shape[0]\n",
    "y = boston_df.values[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "standard_scaler = StandardScaler()\n",
    "X_train = standard_scaler.fit_transform(X_train)\n",
    "X_test = standard_scaler.transform(X_test)\n",
    "y = y.reshape(1, y.shape[0])\n",
    "y_train = y_train.reshape(1, y_train.shape[0])\n",
    "y_test = y_test.reshape(1, y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 13)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "boston_neural_network = NeuralNetwork(\n",
    "    layer_sizes=[13,15,1],\n",
    "    learning_rate=0.2,\n",
    "    is_classifier=False)\n",
    "\n",
    "boston_neural_network.fit(X_train, y_train, epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Plotting Boston neural network error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "858c8dfa65d84a2f90d01a51c752b239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'type': 'scatter',\n",
       "              'uid': '6abb282c-bfe9-42dd-9a09-8f697caf822a',\n",
       " …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = go.FigureWidget()\n",
    "fig.add_scatter(\n",
    "    x=list(range(len(boston_neural_network.costs))),\n",
    "    y=boston_neural_network.costs)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Neural Network accuracy on Boston data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy (R2 score) = 87.17%\n"
     ]
    }
   ],
   "source": [
    "y_pred = boston_neural_network.predict(X_test)\n",
    "test_acc = r2_score(y_test[0], y_pred[0])\n",
    "print(f'Test accuracy (R2 score) = {test_acc:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24.92937298, 20.00371964, 23.04416689, 21.03451595, 17.72778921]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25.2, 19.3, 20.5, 21.2, 21.8]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Neural network classifier - mnist data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### load mnist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "files = [\n",
    "    'train-images-idx3-ubyte.gz',\n",
    "    'train-labels-idx1-ubyte.gz',\n",
    "    't10k-images-idx3-ubyte.gz',\n",
    "    't10k-labels-idx1-ubyte.gz']\n",
    "\n",
    "path='../mnist'\n",
    "path = pathlib.Path(path)\n",
    "    \n",
    "def download_mnist(path):\n",
    "    path.mkdir(exist_ok=True)\n",
    "    url = 'http://yann.lecun.com/exdb/mnist/'\n",
    "    for file in files:\n",
    "        if file not in [file.name for file in path.iterdir()]:\n",
    "            urlretrieve(url + file, path /  file)\n",
    "            print(f\"Downloaded {file} to {path}\")\n",
    "\n",
    "def images(path):\n",
    "    \"\"\"Return images loaded locally.\"\"\"\n",
    "    with gzip.open(path) as f:\n",
    "        pixels = np.frombuffer(f.read(), 'B', offset=16)\n",
    "    return pixels.reshape(-1, 784).astype('float32') / 255\n",
    "\n",
    "def labels(path):\n",
    "    \"\"\"Return labels loaded locally.\"\"\"\n",
    "    with gzip.open(path) as f:\n",
    "        # First 8 bytes are magic_number, n_labels\n",
    "        integer_labels = np.frombuffer(f.read(), 'B', offset=8)\n",
    "    return integer_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train = images(path / files[0])\n",
    "y_train = labels(path / files[1])\n",
    "X_test = images(path / files[2])\n",
    "y_test = labels(path / files[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Visualise mnist samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def display_sample(sample_id, X_train, y_train):\n",
    "    pixels = 28\n",
    "    sample = X_train[sample_id].reshape((pixels,pixels))\n",
    "    y = y_train[sample_id]\n",
    "    print(y)\n",
    "    fig = go.FigureWidget()\n",
    "    fig.add_heatmap(\n",
    "        z=sample,\n",
    "        showscale=False,\n",
    "        colorscale=[\n",
    "            [0, \"rgb(255, 255, 255)\"],\n",
    "            [1.0, \"rgb(0, 0, 0)\"]])\n",
    "    fig.layout.yaxis.autorange = \"reversed\"\n",
    "    fig.layout.xaxis.visible=False\n",
    "    fig.layout.yaxis.visible=False\n",
    "    fig.layout.title = f'Sample {y} of digits'\n",
    "    return fig\n",
    "\n",
    "\n",
    "def display_samples(X_train,\n",
    "                   n_rows = 5,\n",
    "                   n_columns = 10):\n",
    "    pixels = 28\n",
    "    samples = np.ones(shape=(pixels * n_rows, pixels * n_columns))\n",
    "    for i, (row, col) in enumerate(product(range(n_rows),range(n_columns))):\n",
    "        samples[row*28:(row+1)*28,col*28:(col+1)*28] = \\\n",
    "            X_train[i].reshape(pixels, pixels)\n",
    "    fig = go.FigureWidget()\n",
    "    fig.add_heatmap(\n",
    "        z=samples,\n",
    "        showscale=False,\n",
    "        colorscale=[\n",
    "            [0, \"rgb(255, 255, 255)\"],\n",
    "            [1.0, \"rgb(0, 0, 0)\"]])\n",
    "    fig.layout.yaxis.autorange = \"reversed\"\n",
    "    fig.layout.xaxis.visible=False\n",
    "    fig.layout.yaxis.visible=False\n",
    "    fig.layout.title = 'Sample of digits'\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc991cf51f474334988fabdda8cef97f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'colorscale': [[0, 'rgb(255, 255, 255)'], [1.0, 'rgb(0, 0, 0)']],\n",
       "              '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_sample(4, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c28f8a0e6e44252a4a4ee4bd2579d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'colorscale': [[0, 'rgb(255, 255, 255)'], [1.0, 'rgb(0, 0, 0)']],\n",
       "              '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_samples(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Fit neural network to mnist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mnist_network = NeuralNetwork(\n",
    "    layer_sizes=[28*28,400,10],\n",
    "    learning_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One hot encoding Y\n"
     ]
    }
   ],
   "source": [
    "mnist_network.fit(X_train, y_train, epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Plot the cost against epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe0a33cae4ef4c12bf0a0bee56e87160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'type': 'scatter',\n",
       "              'uid': 'e9ea711c-415e-4bfc-93f1-9b943633868a',\n",
       " …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = go.FigureWidget()\n",
    "fig.add_scatter(\n",
    "    x=list(range(len(mnist_network.costs))),\n",
    "    y=mnist_network.costs)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Neural network accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8462166666666666"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred = mnist_network.predict(X_train)\n",
    "train_acc = (train_pred == y_train).sum() / y_train.shape[0]\n",
    "train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8411"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = mnist_network.predict(X_test)\n",
    "test_acc = (test_pred == y_test).sum() / y_test.shape[0]\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "430.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
